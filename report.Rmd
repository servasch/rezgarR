---
title: "Untitled"
author: "Rezgar Arabzadeh"
date: "12/8/2021"
output: html_document
fig_caption: yes
---

```{r setup, include=FALSE,warning=FALSE,message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
install.packages("leaflet")
install.packages("corrplot")
install.packages("EnvStats")
install.packages("Ecfun")
install.packages("trend")
install.packages("openair")
install.packages("plot.matrix")
install.packages("forecast")
library(leaflet)
library(corrplot)
library(EnvStats)
library(Ecfun)
library(trend)
library(openair)
library(plot.matrix)
library(forecast)

```

# Introduction and Motivation

For decades, data collected from the inflow of wastewater treatment plants (WWTPs) have been recognised as an important source of information for the detection of human diseases and/or drug abuse. Consequently, for the Sars-Cov-2 pandemic as well, multiple studies have found wastewater-based epidemiology (WBE) as a potential tool for the monitoring and management of the disease. The virus signal found in wastewater is the true prevalence information, that is, information on all infected persons in the watershed. However, official reporting and statistics relies on the data derived from individual test programs, which includes only a subset of the overall infection and is a function of the test strategy. The difference to the true number of infections (as determined by prevalence data) is significant due to the high number of asymptomatic and mildly infected patients. Still, the latter information serves as the backbone of SARS-CoV-2 management and is usually denoted as the incidence value, typically given as the 7- or 14-day notification rate of new infections for 100,000 inhabitants.

Despite the differences in the properties of the data (WBE prevalence and test-based incidence), studies have reported a significant statistical agreement between the two, thus indicating the capability of wastewater data for the prediction of COVID-19 incidence data. In terms of pandemic management, this is a profound advantage both as supplement and alternative to individual testing. Furthermore, WBE was found to give a slightly earlier signal as compared to the clinical recognition of SARS-CoV-2. Thus, a mathematical model capable to predict the incidence values from the wastewater signal is a valuable tool in pandemic management.

Whereas a simple statistical model, such as a linear regression, can correlate two signals, a thorough analysis of the mechanisms and assumptions of the models are key for a robust prediction. For instance, in addition to the observed latency between two signals, it was shown that the inclusion of metadata on the amount of testing might have a positive effect on the model performance. It is evident that increasing the number of tests results in the identification of a higher number of positive cases. However, owing to a lack of comprehensive community monitoring; untested asymptomatic individuals, who comprise a significant number of patients; and data security concerns, detailed test data—allowing for prevalence estimation—is unavailable on a general basis, thereby restricting the development of rigorous statistical models.

Wastewater-based epidemiology is a recognized source of information for pandemic management. In this study, the correlation between a SARS-CoV-2 signal derived from wastewater sampling, COVID-19 incidence values monitored by means of individual testing programs, and Google mobility data sets are investigated. The dataset used in the study is composed of timelines (duration approx. 14 months) at a wastewater treatment plant in Liechtenstein. 180 SARIMA regression models were investigated to predict the viral incidence under varying data inputs.

# Data and case study
```{r, include=FALSE}
EPD<-read.csv("./datasets/EPD.csv",row.names = 1) # epidemiological data
```
The datasets starts from  from `r rownames(EPD)[1]` and ends in `r rownames(EPD)[nrow(EPD)]`, covering a range of approx. 14 months of weekly wastewater, epidemiological  and mobility time-series. The figure below generated by the Fig.1 chunk code generates the wastewater time-series (red dotted line) superimposed by the incidence values (grey bars), here active cases, and vaccination status (green shaded boxes). All time series 

```{r Fig.1, fig.align="center",fig.cap="\\label{fig:figs}Time series"}
EPD<-read.csv("./datasets/EPD.csv",row.names = 1) # epidemiological data
GMD<-read.csv("./datasets/GMD.csv",row.names = 1) # Google mobility data
SAR<-read.csv("./datasets/SAR.csv",row.names = 1) # waste water data

# Calculating active cases: Confirmed - death - recovered
activeCases<-EPD$confirmed-EPD$deaths-EPD$recovered

# plot time series
tiff('SARIMA01.tiff', units="in", width=5, height=4, res=300, compression = 'lzw')
par(mar = c(5, 4, 4, 4) + 0.3)
labels<-rep("",length(activeCases))
labels[seq(1,nrow(SAR),4)]<-rownames(SAR)[seq(1,nrow(SAR),4)]
barplot(activeCases,axe=FALSE,xlab="",
        ylab="Active Cases",border=NA,names.arg=labels,las=2)
axis(side=2, at = pretty(range(activeCases)))
par(new = TRUE)
plot(SAR[,3],axes = FALSE,bty = "n",typ="o",
     xlab = "", ylab = "",col="red",pch=19)
axis(side=4, at = pretty(range(SAR[,3])))
mtext("SarsCov2 Titer [copies/ml]", side=4, line=3)
polygon(x=c(1:nrow(EPD),nrow(EPD):1),y=c(EPD[,4],rep(0,nrow(EPD))),
        col=adjustcolor("#99CCCC",0.5),border=NA)
polygon(x=c(1:nrow(EPD),nrow(EPD):1),y=c(EPD[,5],rep(0,nrow(EPD))),
        col=adjustcolor("#336666",0.5),border=NA)
dev.off()
```

The location of wastewater treatment plant is depicted by a dynamic map using leaflet in the figure below. The waster water site is located in 9.5 E, 47.2 N which the wastewater of around 37000 population living in Liechtenstein drains here. 

```{r Fig.2, fig.align="center",fig.cap="\\label{fig:figs}Wastewater treatment plant location" }
leaflet() %>%
addTiles() %>%
addMarkers(lng=9.501531, lat=47.214083, popup="The birthplace of R")
```

## policy and restriction time series
In addition to the data resources introduced above, we will be using the policy and restriction factors deployed in Liechtenstein to control the pandemic. The figure below shows the restriction/policy level deployed in Liechtenstein to control the pandemic. Obviously the higher value the stricter/harder policy. 

```{r Fig.3, fig.align="center",fig.cap="\\label{fig:figs}COVID-19 related restricts and policies"}
tiff('SARIMA02.tiff', units="in", width=10, height=4, res=300, compression = 'lzw')
col<-colorRampPalette(c("white","red"))
par(mai=c(1.5,3.5,0.5,1.6))
plot(t(EPD[,-(1:5)]),main="degree of restriction",xlab="",ylab="",col=col,las=2)
dev.off()
```

## Correlation analysis
Since multiple sources of time series/datasets are used to model/predict the Covid-19 prevalence, it might be worth to look in a simple Spearman correlation plot before diving in the modelling and data analysis. The correlogram shown in the figure below, shows the correlation value and magnitude for all pairs of time series used in this study. To simplify the labeling, an alphabetic labels are used for the time series in the study. Zt, stands for the Covid-19 incidence in the WWTP watershed. The Table followed by the Figure shows the labels and their original equivalent ones.

```{r Fig.4, fig.align="center",fig.cap="\\label{fig:figs}Corrologram"}
tiff('SARIMA03.tiff', units="in", width=10, height=4, res=300, compression = 'lzw')
data<-data.frame(activeCase=activeCases,
                 SarsCov2Load=SAR$SarsCoV2_Load_.Giga_copied.day...copies.ml.,
                 GMD=GMD,EPD[,-c(1:3)])
labels<-c("zt",letters[1:(ncol(data)-1)])
originalLabels<-colnames(data)
colnames(data)<-labels
M<-cor(data,method = "spearman") # spearman correlation coefficient
corrplot(M, method = 'ellipse', type = 'lower',col = COL2('PiYG'))
dev.off()
```
```{r,eval=TRUE,echo=FALSE}
labels<-labels[order(nchar(originalLabels),decreasing = TRUE)]
originalLabels<-originalLabels[order(nchar(originalLabels),decreasing = TRUE)]
data.frame(cbind(labels,originalLabels))
```

According to the correlogram, the variables with a significant correlation with Zt are: `r originalLabels[match(names(which(sort(abs(M[-1,1]),decreasing = TRUE)>0.1)),labels)]`. 

# Methods and Modelling Strategy
In this study a families of ARIMA models will be checked and evaluated. Upon a very intermediate time  checking a SARIMA model may fits the response variable time series, as it is signifying a sort of periodic behaviors. However, we will get into this in the next section.
The modelling will be conducted with three different approaches and objectives:

* Single variable model (Zt)
* Single variable model (Zt) with Google mobility data as exogenous variable 
* Single variable model (Zt) with Google mobility and policy time-series as exogenous variable 

In the firs approach, a best plausible model structure is identified using a validation subset. This is in contrast with the traditional model selection using the ACFs. To do so, we will identify a very pleminary model structure using ACF and PACF diagrams then we will perturb the model parameters around the selected structures. In the next step for any pair of perturbed model parameters, a model diagnostic check will be performed for a prediction horizon. This is conducted as many studies have shown the possibility of over fitting with model selection using calibration datasets.

In the second strategy, the same methodology will be deployed as in the first strategy except two mobility time-series, percentage change of transit and workplace, are used as the exogenous variables.

In the third strategy, the target is to identify the effectiveness of policies and variables on the model response, incidences. The model selection approach is the same as the other strategies, however, a very simple sensitivity analysis is conducted to find the most sensitive exogenous variables.


# Results
## time series preprocessing
Any modelling with classic time series tools, i.e. ARIMA model, requires stationarity and normality in the time-series. As a very first step, the trend in the time series is checked using a Mann-Kendall trend test. The null hypothesis in the MK test to trend is: the time series has no trend and the alternative one is vice versa. the code chunk below will takes care of this and will detrend the time series using a first order differentiation if necessary. The significance level has set to 95%. 
```{r}
p.value<-mk.test(data$zt)$p.value # Mann-Kendall trend test
zt<-data$zt
if(p.value>0.05)
{
   zt<-diff(zt)
}
zt<-diff(zt,lag=12)
```

The MK trend test showed that the time series requires differentiation. Given that the time series trend is stabilized using the diff function, then we will go with the normality checking. A very straightforward to check the data normality can be via histogram or qqplot. the chuck code below will take care of this and returns a figure of two panels showing both digrams. 

```{r Fig.5, fig.align="center",fig.cap="\\label{fig:figs} Normality checks"}
# normality tests and plots
tiff('SARIMA04.tiff', units="in", width=10, height=4, res=300, compression = 'lzw')
par(mfrow=c(1,2))
hist(zt,prob=TRUE,main="SarsCov2 Prevelance",xlab="Prevelance")
lines(density(zt,50))
qqnorm(zt)
qqline(zt)
p.value<-shapiro.test(data$zt)$p.value
legend("topleft",
       legend=c("Shapiro-Wilk Test",
                sprintf("p.value= %f",p.value)),
       bty="n",cex=0.7)
dev.off()
```

As it is clear from the both histogram and qqplot as well as Shapiro-wilk test, the time series is not normally distributed and requires further manipulation. In order to normalize the time-series, a BoxCox transformation is used to normalize the dataset. The BoxCox parameter, lambda, will be optimized using a very simple grid search with the precision of 0.01. Once we have the BoxCox transformed time series, we can plot the ACF and PACF. t
he code chunk below computes the method parameter and will plot the transformed time-series ACF plots.
```{r Fig.6, fig.align="center",fig.cap="\\label{fig:figs} ACF and PACF of the transformed timeseries"}
# 95% confidence >> alpha = 1.00-0.95
tiff('SARIMA05.tiff', units="in", width=6, height=4, res=300, compression = 'lzw')
if(p.value<0.05)
{
   #boxcox parameters optimization
   bxc<-EnvStats::boxcox(data$zt,lambda=seq(-2,2,0.01))
   plot(bxc,main="")
   lambda<-bxc$lambda[which.max(bxc$objective)]
   abline(v=lambda,lwd=2,col="red")
   offset<-min(zt)
   zt<-zt-offset
   zt<-((zt^lambda)-1)/lambda #boxcox transformation
}
dev.off()

# plot ACF & PAC
tiff('SARIMA06.tiff', units="in", width=10, height=4, res=300, compression = 'lzw')
par(mfrow=c(1,2))
acf (zt,main="",length(zt)/4)
pacf(zt,main="",length(zt)/4)
dev.off()
```

## Univariate modelling: First strategy

As seen from the ACF, there are evidences of periodic behavior in the time series and at lags 1 and 12 the ACF values crosses the 95% significant levels. However, the PACF spikes at lag 8 corresponding to a full period of cycle in the time series (two months). To find a best model structure the parameter of seasonal and arima components are explored in a wide range of plausible values using the chunk code blow. Then every model structures are checked for the validation period using a range of model diagnostics, i.e. RMS and correlation.
```{r, warning=FALSE}
# copying original data
newData<-data
newData$zt<-((newData$zt^lambda)-1)/lambda

# creating plausible model choices
p<-0:2
q<-0:2
P<-0:2
Q<-0:2
modelChoices<-expand.grid(p,q,P,Q)
AIC<-rep(NA,nrow(modelChoices))
CalibrationPeriod<-1:round(0.8*nrow(data))
validationPeriod<-(1:nrow(data))[-CalibrationPeriod]
diagMatrix<-matrix(NA,nrow(modelChoices),2)
zhat<-matrix(NA,nrow(modelChoices),length(validationPeriod))
rownames(zhat)<-paste("model",1:nrow(zhat))

for(i in 1:nrow(modelChoices))
{
   p<-modelChoices[i,1]
   q<-modelChoices[i,2]
   P<-modelChoices[i,3]
   Q<-modelChoices[i,4]
   out<-tryCatch(expr=arima(x=newData$zt[CalibrationPeriod],order=c(p,1,q),
                            seasonal =list(order = c(P,1,Q), period = 12)),
                 error   = function(e){TRUE})
   if(class(out)=="Arima")
   {
      model<-out
      AIC[i]<-model$aic
      zhat[i,]<-forecast(model,length(validationPeriod))$mean
      Zobs<-newData$zt[validationPeriod]
      diagMatrix[i,1]<-cor(zhat[i,],Zobs)
      diagMatrix[i,2]<-(sum((zhat[i,]-Zobs)^2)/(length(zhat[i,])-1))^0.5
   }
}
```

According to the simulation experiments performed, the AIC metrics for evaluated model structures are drawn using the chunk code below. According to the AIC diagram, the model diagnostic metric drops as it gets complicated and from a particular complexity level the AIC starts degrading. Here, we have chosen the top 5 percentile AIC equivalent model structures as the candidates. Then among them the model structure is selected as the best with the least level of complexity.Then the selected model is evaluated for both calibration and validation horizons. As seen from the figure, the model predictions for the  validation period matches fairly well with the observations, however, given this prediction model diagnostic metrics such as correlation is not expected to be in a satisfactory level.

```{r Fig.7, fig.align="center",fig.cap="\\label{fig:figs} AIC metric for calibration period"}
# drawing best 5% AIC scores for calibration data and the best selected model forecast
tiff('SARIMA07.tiff', units="in", width=6, height=4, res=300, compression = 'lzw')
#par(mfrow=c(1,2))
plot(AIC,xlab="model choices")
abline(h=quantile(AIC,0.05,na.rm = TRUE),lwd=2,col="red",lty=2)
dev.off()

# picking the best model structure using calobration AIC among the top 5% models
candidates<-modelChoices[AIC<quantile(AIC,0.05,na.rm=TRUE),]
bestChoice<-as.numeric(names(which.min(apply(candidates,1,sum))))

# simulating the best selected model based on calibration data
tiff('SARIMA08.tiff', units="in", width=6, height=4, res=300, compression = 'lzw')
params<-as.matrix(modelChoices[bestChoice,])
model<-arima(x=newData$zt[CalibrationPeriod],order=c(params[1],1,params[2]),
      seasonal =list(order = c(params[3],1,params[4]), period = 8))
plot(forecast(model,length(validationPeriod),0.95),main="",xlab="Time step (week)",ylab="incidence")
lines(validationPeriod,newData$zt[validationPeriod],lty="dashed")
dev.off()
```

Despite the pretty acceptable performance of the SARIMA model, it might be beneficial to look into the diagnostic metrics for the validation horizon while selecting  model structure. 

```{r Fig.8, fig.align="center",fig.cap="\\label{fig:figs} Taylor diagram and selcted model based on the validation set"}
tiff('SARIMA09.tiff', units="in", width=6, height=4, res=300, compression = 'lzw')
#par(mfrow=c(1,2))
# plot Taylor diagram for model selection
models<-c(t(matrix(rep(paste("model",1:nrow(modelChoices)),
     length(validationPeriod)),nrow(modelChoices),length(validationPeriod))))
dat<-data.frame(obs=rep(Zobs,nrow(modelChoices)),mod=c(zhat),group=models)
missingModels<-paste("model",which(is.na(diagMatrix[,1])))
for(i in 1:length(missingModels)) dat<-dat[!(dat$group==missingModels[i]),]
TaylorDiagram(mydata = dat,obs = "obs",mod="mod",group="group")
rownames(diagMatrix)<-1:nrow(diagMatrix)
dev.off

# Model selection using a two filter stage: RMS and Correlation filtering
RMSBasedCandid<-diagMatrix[diagMatrix[,2]<
                  quantile(diagMatrix[,2],0.05,na.rm=TRUE),,drop=FALSE]
CorrRMSBest<- RMSBasedCandid[RMSBasedCandid[,1]>
                  quantile(RMSBasedCandid[,1],0.95,na.rm=TRUE),,drop=FALSE]
CorrRMSBest<-CorrRMSBest[!is.na(rownames(CorrRMSBest)),,drop=FALSE]
topModels<-modelChoices[match(rownames(CorrRMSBest),rownames(modelChoices)),]
params<-as.matrix(topModels[which.min(apply(topModels,1,sum)),])
model<-arima(x=newData$zt[CalibrationPeriod],order=c(params[1],1,params[2]),
      seasonal =list(order = c(params[3],1,params[4]), period = 12))

# plot selected model outputs based on validation set
tiff('SARIMA10.tiff', units="in", width=6, height=4, res=300, compression = 'lzw')
plot(forecast(model,length(validationPeriod),0.95),main="",xlab="time step",ylab="incidences")
lines(validationPeriod,newData$zt[validationPeriod],lty="dashed")
dev.off()
```

## Univariate modelling with Google mobility data: Second strategy

As described before in this section, an experiment will be conducted to explore in the model structure space to find a optimum model configuration based on the validation set. All experiments are performed using the below chunk codes and the best model configuration is selected. Then the model performance, i.e. fitness, is drawn for both evaluation time horizons as shown in the figure below. According the figure the fitness has improved significantly compared with the univariate model without exogenous variables - indicating the implication of social distancing and mobility data in the prediction of the fate of virus.

```{r, warning=FALSE}
# copying original data
newData<-data
newData$zt<-((newData$zt^lambda)-1)/lambda

# creating plausible model choices
p<-0:2
q<-0:2
P<-0:2
Q<-0:2
modelChoices<-expand.grid(p,q,P,Q)
AIC<-rep(NA,nrow(modelChoices))
CalibrationPeriod<-1:round(0.8*nrow(data))
validationPeriod<-(1:nrow(data))[-CalibrationPeriod]
diagMatrix<-matrix(NA,nrow(modelChoices),2)
zhat<-matrix(NA,nrow(modelChoices),length(validationPeriod))
rownames(zhat)<-paste("model",1:nrow(zhat))

for(i in 1:nrow(modelChoices))
{
   p<-modelChoices[i,1]
   q<-modelChoices[i,2]
   P<-modelChoices[i,3]
   Q<-modelChoices[i,4]
   out<-tryCatch(expr=arima(x=newData$zt[CalibrationPeriod],order=c(p,1,q),
                            xreg = newData[CalibrationPeriod,c("b","c")],
                            seasonal =list(order = c(P,1,Q), period = 12)),
                 error   = function(e){TRUE})
   if(class(out)=="Arima")
   {
      model<-out
      AIC[i]<-model$aic
      zhat[i,]<-predict(model,n.ahead = length(validationPeriod),
                        newxreg =newData[validationPeriod,c("b","c")])$pred
      Zobs<-newData$zt[validationPeriod]
      diagMatrix[i,1]<-cor(zhat[i,],Zobs)
      diagMatrix[i,2]<-(sum((zhat[i,]-Zobs)^2)/(length(zhat[i,])-1))^0.5
   }
}
```
```{r Fig.9, fig.align="center",fig.cap="\\label{fig:figs} Model fitness with support of google Mobility data"}
rownames(diagMatrix)<-1:nrow(diagMatrix)
RMSBasedCandid<-diagMatrix[diagMatrix[,2]<
                  quantile(diagMatrix[,2],0.05,na.rm=TRUE),,drop=FALSE]
CorrRMSBest<- RMSBasedCandid[RMSBasedCandid[,1]>
                  quantile(RMSBasedCandid[,1],0.95,na.rm=TRUE),,drop=FALSE]
CorrRMSBest<-CorrRMSBest[!is.na(rownames(CorrRMSBest)),,drop=FALSE]
topModels<-modelChoices[match(rownames(CorrRMSBest),rownames(modelChoices)),]
params<-as.matrix(topModels[which.min(apply(topModels,1,sum)),])
model<-arima(x=newData$zt[CalibrationPeriod],order=c(params[1],1,params[2]),
             xreg = newData[CalibrationPeriod,c("b","c")],
             seasonal =list(order = c(params[3],1,params[4]), period = 12))

# plot selected model outputs based on validation set
zhatValid<-predict(model,n.ahead=length(validationPeriod),
          newxreg=newData[validationPeriod,c("b","c")])$pred
zhatCalib<-fitted(model)
plot(newData$zt,typ='l',ylab="incidence",xlab="Time step",ylim=range(zhatCalib))
lines(CalibrationPeriod,zhatCalib ,col="red")
lines(validationPeriod,zhatValid,col="red",lty="dashed")
```

## Sensitivity analysis of univariate model to the exegenous variables: Third strategy
In this strategy, the target is measuring the sensitivity of the model respect to the exogenous variables. The experiments are performed like the previous strategies expect the experiments are performed equal to the number of exogenous variables in which for given trial the model is evaluated across all plausible structures under the absence of an exogenous variable. The procedure is repeated for all variables. 

```{r, warning=FALSE,eval=FALSE}
##### WARNING: very computationally expensive chunk
# copying original data
newData<-data
newData$zt<-((newData$zt^lambda)-1)/lambda
newData<-newData[,-ncol(newData)]

# creating plausible model choices
p<-0:1
q<-0:1
P<-0:1
Q<-0:1
modelChoices<-expand.grid(p,q,P,Q)
rownames(modelChoices)<-1:nrow(modelChoices)
AIC<-matrix(NA,nrow(modelChoices),ncol(newData)-1)
CalibrationPeriod<-1:round(0.8*nrow(data))
validationPeriod<-(1:nrow(data))[-CalibrationPeriod]
diagMatrix<-array(NA,c(nrow(modelChoices),2,ncol(newData)-1))
zhat<-array(NA,c(nrow(modelChoices),length(validationPeriod),ncol(newData)-1))

for(j in 2:ncol(newData))
{
  for(i in 1:nrow(modelChoices))
  {
     dat<-newData[,-j]
     p<-modelChoices[i,1]
     q<-modelChoices[i,2]
     P<-modelChoices[i,3]
     Q<-modelChoices[i,4]
     xreg   <-dat[CalibrationPeriod,-1]
     newxreg<-dat[validationPeriod   ,-1]
     out<-tryCatch(expr=arima(x=dat$zt[CalibrationPeriod],order=c(p,1,q),
                      seasonal =list(order = c(P,1,Q), period = 12),
                      xreg = xreg),
                   error   = function(e){TRUE})
     if(class(out)=="Arima")
     {
        model<-out
        AIC[i,j-1]<-model$aic
        zhat[i,,j-1]<-predict(object=model,n.ahead=length(validationPeriod),
                          newxreg=newxreg)$pred
        Zobs<-dat$zt[validationPeriod]
        diagMatrix[i,1,j-1]<-cor(zhat[i,,j-1],Zobs)
        diagMatrix[i,2,j-1]<-(sum((zhat[i,,j-1]-Zobs)^2)/(length(zhat[i,,j-1])-1))^0.5
     }
  }
}
```

# Conclusion and Remarks
According to the experiments conducted to predict virus fate in the Liechtenstein WWTP the followings can be concluded:

* prediction of incidences with a univariate model for a long leads, i.e. three months, is hard
* Google mobility data showed that they can enhance the model accuracy significantly and it is recommended to include them in the models where they are available
* Based on the experiments, the results showed that `r originalLabels[match(c("a","f","i","j"),labels)]` are the most important policies affecting the Covid-19 prevalence.

